"""
Analysis commands for AIchemist Archivum CLI.

This module provides commands for analyzing content, generating metrics,
building relationship graphs, and extracting insights from the codebase.
"""

import asyncio
from pathlib import Path
from typing import Annotated

import typer
from rich.console import Console
from rich.progress import Progress
from rich.table import Table

from ..cli import analyze_app

console = Console()


@analyze_app.command("code")
def analyze_code(
    path: Annotated[
        Path,
        typer.Argument(
            help="Path to file or directory to analyze",
            exists=True,
        ),
    ],
    depth: Annotated[
        int,
        typer.Option(
            "--depth", "-d", help="Analysis depth (1-3, higher = more detailed)"
        ),
    ] = 2,
    include_metrics: Annotated[
        bool, typer.Option("--metrics", "-m", help="Include complexity metrics")
    ] = True,
) -> None:
    """
    ðŸ“Š Analyze code structure and complexity

    Performs comprehensive code analysis including complexity metrics,
    function/class detection, and code quality indicators.
    """
    asyncio.run(_analyze_code_async(path, depth, include_metrics))


async def _analyze_code_async(path: Path, depth: int, include_metrics: bool) -> None:
    """Async implementation of code analysis."""
    console.print(f"ðŸ“Š [bold cyan]Analyzing code:[/bold cyan] {path}")

    try:
        from aichemist_archivum.core.analysis.technical_analyzer import process_file

        results = {}

        if path.is_file():
            # Analyze single file
            with Progress() as progress:
                task = progress.add_task("Analyzing file...", total=100)
                progress.update(task, advance=50)

                file_type, analysis = await process_file(path)
                results = {
                    "files_analyzed": 1,
                    "file_type": file_type,
                    "analysis": analysis,
                }

                progress.update(task, advance=50)
        else:
            # Analyze directory
            with Progress() as progress:
                task = progress.add_task("Scanning directory...", total=100)
                progress.update(task, advance=30)

                # Count Python files
                py_files = list(path.rglob("*.py"))
                results["files_analyzed"] = len(py_files)

                progress.update(task, advance=40, description="Analyzing files...")

                # Analyze a sample or all depending on depth
                sample_size = min(depth * 10, len(py_files))
                total_loc = 0
                total_complexity = 0
                functions = 0
                classes = 0

                for py_file in py_files[:sample_size]:
                    try:
                        _, analysis = await process_file(py_file)
                        if "metrics" in analysis:
                            total_loc += analysis["metrics"].get("loc", 0)
                            total_complexity += analysis["metrics"].get("complexity", 0)
                        if "structure" in analysis:
                            functions += len(analysis["structure"].get("functions", []))
                            classes += len(analysis["structure"].get("classes", []))
                    except Exception:
                        pass

                results.update(
                    {
                        "lines_of_code": total_loc,
                        "functions": functions,
                        "classes": classes,
                        "complexity_avg": total_complexity / max(sample_size, 1),
                    }
                )

                progress.update(task, advance=30, description="Complete!")

        console.print("âœ… [green]Analysis completed![/green]")

        # Display results
        table = Table(show_header=False, box=None)
        table.add_column("Metric", style="cyan", width=25)
        table.add_column("Value", style="white")

        if path.is_file() and "analysis" in results:
            analysis = results["analysis"]
            table.add_row("File Type", results.get("file_type", "unknown"))

            if "metrics" in analysis:
                metrics = analysis["metrics"]
                table.add_row("Lines of Code", str(metrics.get("loc", 0)))
                table.add_row("Complexity", f"{metrics.get('complexity', 0):.1f}")
                table.add_row("Assessment", metrics.get("assessment", "N/A"))

            if "structure" in analysis:
                struct = analysis["structure"]
                table.add_row("Functions", str(len(struct.get("functions", []))))
                table.add_row("Classes", str(len(struct.get("classes", []))))
                table.add_row("Imports", str(len(struct.get("imports", []))))
        else:
            for key, value in results.items():
                if key != "analysis":
                    formatted_value = (
                        f"{value:.1f}" if isinstance(value, float) else str(value)
                    )
                    table.add_row(key.replace("_", " ").title(), formatted_value)

        console.print(table)

    except Exception as e:
        console.print(f"âŒ [red]Error during analysis:[/red] {e}")
        import traceback

        console.print(f"[dim]{traceback.format_exc()}[/dim]")


@analyze_app.command("relationships")
def analyze_relationships(
    path: Annotated[
        Path | None, typer.Argument(help="Path to analyze (optional)")
    ] = None,
    relationship_type: Annotated[
        str,
        typer.Option(
            "--type", "-t", help="Type of relationships to analyze (imports/calls/all)"
        ),
    ] = "all",
) -> None:
    """
    ðŸ•¸ï¸  Analyze relationships between files and components

    Builds relationship graphs showing how files and modules are connected.
    """
    asyncio.run(_analyze_relationships_async(path, relationship_type, 2))


async def _analyze_relationships_async(
    path: Path | None, relationship_type: str, depth: int
) -> None:
    """Async implementation of relationship analysis."""
    console.print("ðŸ•¸ï¸  [bold cyan]Analyzing relationships...[/bold cyan]")

    try:
        # Use provided path or default to current directory
        target_path = path or Path(".")
        console.print(f"ðŸ“ Target: {target_path}")

        with Progress() as progress:
            task = progress.add_task("Analyzing relationships...", total=100)
            progress.update(task, advance=30)

            relationships = {}

            if target_path.is_file():
                # Analyze single file for imports
                console.print(f"Analyzing file: {target_path.name}")
                try:
                    from aichemist_archivum.core.analysis.technical_analyzer import (
                        process_file,
                    )

                    _, analysis = await process_file(target_path)

                    if "structure" in analysis:
                        imports = analysis["structure"].get("imports", [])
                        relationships[str(target_path)] = {
                            "imports": imports,
                            "count": len(imports),
                        }
                except Exception as e:
                    console.print(f"[dim]Could not analyze {target_path}: {e}[/dim]")
            else:
                # Analyze directory
                py_files = list(target_path.rglob("*.py"))[: depth * 5]

                progress.update(task, advance=30, description="Scanning files...")

                for py_file in py_files:
                    try:
                        from aichemist_archivum.core.analysis.technical_analyzer import (
                            process_file,
                        )

                        _, analysis = await process_file(py_file)

                        if "structure" in analysis:
                            imports = analysis["structure"].get("imports", [])
                            if imports:
                                relationships[py_file.name] = {
                                    "imports": imports[:5],  # Top 5
                                    "count": len(imports),
                                }
                    except Exception:
                        pass

                progress.update(task, advance=40, description="Complete!")

        console.print("âœ… [green]Analysis completed![/green]")

        if not relationships:
            console.print("ðŸ“­ [dim]No relationships found[/dim]")
            return

        # Display results
        table = Table(title="File Relationships")
        table.add_column("File", style="cyan", width=30)
        table.add_column("Imports", style="yellow")
        table.add_column("Count", style="green", justify="right")

        for file_name, data in list(relationships.items())[:20]:  # Limit to 20
            imports_str = ", ".join(data["imports"][:3])
            if len(data["imports"]) > 3:
                imports_str += "..."
            table.add_row(
                file_name if len(file_name) < 30 else file_name[:27] + "...",
                imports_str or "-",
                str(data["count"]),
            )

        console.print(table)
        console.print(f"\nðŸ“Š Total files analyzed: {len(relationships)}")

    except Exception as e:
        console.print(f"âŒ [red]Error during relationship analysis:[/red] {e}")
        import traceback

        console.print(f"[dim]{traceback.format_exc()}[/dim]")


@analyze_app.command("metrics")
def generate_metrics(
    path: Annotated[
        Path | None, typer.Argument(help="Path to analyze (optional)")
    ] = None,
    metric_types: Annotated[
        list[str] | None,
        typer.Option("--type", "-t", help="Types of metrics to generate"),
    ] = None,
) -> None:
    """
    ðŸ“ˆ Generate comprehensive metrics and statistics

    Produces detailed metrics about code quality, complexity, and size distribution.
    """
    if metric_types is None:
        metric_types = ["size", "complexity", "quality"]
    asyncio.run(_generate_metrics_async(path, metric_types))


async def _generate_metrics_async(path: Path | None, metric_types: list[str]) -> None:
    """Async implementation of metrics generation."""
    scope = str(path) if path else "entire project"
    console.print(f"ðŸ“ˆ [bold cyan]Generating metrics for:[/bold cyan] {scope}")

    # Mock metrics data
    metrics = {
        "size": {"total_files": 45, "total_lines": 12845, "avg_file_size": 285.4},
        "complexity": {"avg_cyclomatic": 3.7, "max_complexity": 12},
        "quality": {"maintainability_index": 78.5, "test_coverage": 85.2},
    }

    console.print("âœ… [green]Metrics generated![/green]")

    for metric_type in metric_types:
        if metric_type in metrics:
            console.print(f"\nðŸ“Š [bold]{metric_type.title()} Metrics:[/bold]")

            table = Table(show_header=False, box=None)
            table.add_column("Metric", style="cyan", width=25)
            table.add_column("Value", style="white")

            for key, value in metrics[metric_type].items():
                formatted_value = (
                    f"{value:.1f}" if isinstance(value, float) else str(value)
                )
                table.add_row(key.replace("_", " ").title(), formatted_value)

            console.print(table)


@analyze_app.command("tech-stack")
def analyze_tech_stack(
    path: Annotated[
        Path | None, typer.Argument(help="Path to analyze (optional)")
    ] = None,
    include_versions: Annotated[
        bool, typer.Option("--versions", "-v", help="Include dependency versions")
    ] = False,
) -> None:
    """
    ðŸ› ï¸  Analyze technology stack and dependencies

    Identifies programming languages, frameworks, and libraries used in the project.
    """
    asyncio.run(_analyze_tech_stack_async(path, include_versions))


async def _analyze_tech_stack_async(path: Path | None, include_versions: bool) -> None:
    """Async implementation of tech stack analysis."""
    scope = str(path) if path else "entire project"
    console.print(f"ðŸ› ï¸  [bold cyan]Analyzing tech stack:[/bold cyan] {scope}")

    # Mock tech stack data
    languages = {
        "Python": {"files": 32, "lines": 8954, "percentage": 69.7},
        "JavaScript": {"files": 8, "lines": 2156, "percentage": 16.8},
        "TypeScript": {"files": 5, "lines": 1735, "percentage": 13.5},
    }

    frameworks = [
        {
            "name": "FastAPI",
            "files": 12,
            "version": "0.104.1" if include_versions else None,
        },
        {
            "name": "React",
            "files": 8,
            "version": "18.2.0" if include_versions else None,
        },
        {"name": "Typer", "files": 6, "version": "0.9.0" if include_versions else None},
    ]

    console.print("âœ… [green]Tech stack analysis completed![/green]")

    # Display languages
    console.print("\nðŸ’» [bold]Programming Languages:[/bold]")
    lang_table = Table(show_header=True, header_style="bold cyan")
    lang_table.add_column("Language", style="blue", width=15)
    lang_table.add_column("Files", style="green", width=8)
    lang_table.add_column("Percentage", style="magenta", width=12)

    for lang, data in languages.items():
        lang_table.add_row(lang, str(data["files"]), f"{data['percentage']:.1f}%")

    console.print(lang_table)

    # Display frameworks
    console.print("\nðŸ—ï¸  [bold]Frameworks:[/bold]")
    framework_table = Table(show_header=True, header_style="bold cyan")
    framework_table.add_column("Framework", style="blue", width=20)
    framework_table.add_column("Files", style="green", width=8)
    if include_versions:
        framework_table.add_column("Version", style="yellow", width=12)

    for framework in frameworks:
        row_data = [framework["name"], str(framework["files"])]
        if include_versions and framework["version"]:
            row_data.append(framework["version"])
        framework_table.add_row(*row_data)

    console.print(framework_table)


@analyze_app.command("summary")
def analysis_summary() -> None:
    """
    ðŸ“‹ Show comprehensive analysis summary

    Displays a high-level overview of all analysis results.
    """
    console.print("ðŸ“‹ [bold cyan]Analysis Summary Report[/bold cyan]")

    # Mock summary data
    summary_stats = {
        "project_health": "Good",
        "total_files": 45,
        "total_lines": 12845,
        "code_quality": 78.5,
        "test_coverage": 85.2,
        "main_language": "Python (69.7%)",
    }

    # Health indicators
    table = Table(title="ðŸ¥ Project Health", show_header=False, box=None)
    table.add_column("Metric", style="cyan", width=20)
    table.add_column("Status", style="white")

    for metric, status in summary_stats.items():
        if isinstance(status, str) and status in ["Good", "Low", "Improving"]:
            status = f"[green]{status}[/green]"

        table.add_row(metric.replace("_", " ").title(), str(status))

    console.print(table)

    # Key insights
    console.print("\nðŸ’¡ [bold]Key Insights:[/bold]")
    insights = [
        "âœ… Code quality is above average (78.5/100)",
        "âœ… Test coverage is excellent (85.2%)",
        "âš ï¸ 8 functions have high complexity (>7)",
        "ðŸ“ˆ Code quality trend is improving",
    ]

    for insight in insights:
        console.print(f"  {insight}")
